20140116
So the plan for these experiments is to test building personalized models for each cell.

To make a more managable starting data set I've chosen to look just at division 1. I will output all events for 2000,2001,and 2002.
Output features will be produced for 2001 and 2002 (I need data going back 365 days). I'm just going to base features on total counts (not broken down by crime type).

The 500mx500m grid gives 400 cells of which 231 actually have any crimes recorded. I've taken these as the possible areas crime could occured and discarded the others as I don't have any a-priori boundaries at the sector level (and at this level of aggregation I won't be throwing away many areas where crime could have been reported but wasn't.

There are 74787 events total in this division in this time period.

Now feed the results into eclipse, run that and

VW

vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache --invert_hash model --ignore a --passes 10 
now the error rate running this goes up and then occilates up and down - perhaps just because of the timing of the arrival of data points is not random... maybe I should shuffle them. This magic line above does seem to produce pretty much identical results to R.

vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache --invert_hash read.model --ignore a --passes 10 -f model <- actually spits out the model to use for test

vw -t -d ctest --cache_file /tmp/vw.test.cache -i model -p test.pred --ignore a 
This generates a list of predictions. Then I'll need some utility script to calculate rsme from that ...

but moving on ... I should calculate run the model with quadtratic features 

vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache --invert_hash read.model -q ab  -f model --passes 100 <- actually only takes ~ a minute:-)

vw -t -d ctest --cache_file /tmp/vw.cache.test -q ab -i model -p qf.pred

Now predict and see how well we have done ... just loaded data back into R to plot and calculate RMSE. Sadly the model with quadratic features does slightly worse than the model without (as implmented in R)

Build the baseline model in vw and compare that to the R result.

vw -k -l .5 -d ctrain --cache_file /tmp/vw.train.cache --invert_hash base.read.model --ignore a  -f base.model --passes 100
vw -t -d ctest --cache_file /tmp/vw.cache.test --ignore a -i base.model -p base.pred
Still slightly worse than R performance but better than model with quadratic features...

Switching to -bfgs yeilds identical performance to R for non-quadratic features(although for some reason the readable model doesn't seem to show up)
vw -k -l 5 -d ctrain --cache_file /tmp/vw.train.cache --invert_hash base.read.model --ignore a  -f base.model --passes 10 --holdout_off --bfgs

Lets test --bfgs with the quadratic features - there is reportedly a possible bug here ...

vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache --invert_hash read.model -q ab  -f model --passes 10 --bfgs
vw -t -d ctest --cache_file /tmp/vw.cache.test -q ab -i model -p qf.pred

This yeilds is better than without the --bfgs arguament but a fraction worse than just using the baseline features. (rmse of 174.69 vs 174.35)

20140122
Testing VW personalized model approach on generated data (against regression models in python)
vw -k -l .5 -d dtrain --cache_file /tmp/vw.cache --invert_hash read.model -q ab  -f model --passes 10 --bfgs
vw -t -d dtest --cache_file /tmp/vw.cache.test -q ab -i model -p qf.pred


Setting really high learning rate doesn't seem to do anything bad (at least in my tiny python-demo example - I wonder why not)
Other Questions I could ask ... does predicting burglary from burglary alone do better than from total? Or better from all cross terms together.

20140123
Moved to a 200m grid size. Added in features for each of 10 crime types and premisis types seperately in their own namespaces. Built features for period 2001-2003. 2001 = training, 2002 = validation, 2003 = test. (total data touched = [2000-2003]).We will now need some level of regularization, particularly when cross features are included.I am still only using features based on the amount of crime within the current cell.

If you use a regularized linear model, is there some garentee that with the optimal regularization parameter, there is no penalty for adding additional terms? (Of course the limit on how close you get to the optimal regularization parameter is dependent on the amount of data in the validation set).

POSSIBLE BUG - read.model emty if bfgs is used as opposed to sgd
# start out running baseline - predict crime based on all crime
vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache --invert_hash base.coef -f base.model --passes 50 --keep o
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i base.model -p base.pred
rmse = 0.255788616516

vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache  -f base.model2 --passes 50 --keep o --bfgs
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i base.model2 -p base.pred2
0.255424253015 -> the benchmark number to beat

vw -k -l 2 -d ctrain --cache_file /tmp/vw.cache  -f base.model3 --passes 50 --keep o # higher learning rate does not improve things
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i base.model3 -p base.pred3
0.255887596981


vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache --invert_hash coef1 -f model1 --passes 50 
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model1 -p pred1
rmse = 0.259416995913

vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache -f model2 --passes 50 --bfgs
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model2 -p pred2
0.255744000997 - slightly worse than with only baseline features

vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache -f model3 --passes 50 --bfgs --ignore o
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model3 -p pred3
0.255701616435


BFGS really does seem better than sgd - maybe I should consider explicitly generating my quadratic features at some point

vw -k -l .5 --l2 .5 -d ctrain --cache_file /tmp/vw.cache  -f model4 --passes 50  --ignore o -q a:
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model4 -p pred4 -q a:
0.275278642938

vw -k -l .5 --l2 1 -d ctrain --cache_file /tmp/vw.cache  -f model5 --passes 50  --ignore o -q a:
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model5 -p pred5 -q a:
0.275278642938

vw -k -l .5 --l2 0 -d ctrain --cache_file /tmp/vw.cache  -f model6 --passes 50  --ignore o -q a: L2 REGULARISATION EXPLICITY 0
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model6 -p pred6 -q a:
rmse = 4.1 

vw -k -l .5  -d ctrain --cache_file /tmp/vw.cache  -f model7 --passes 50  --ignore o -q a: No explicit regularisation...
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model7 -p pred7 -q a:
rmse 4.10977945992

vw -k -l .5 --l2 0.1 -d ctrain --cache_file /tmp/vw.cache  -f model8 --passes 50  --ignore o -q a:
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model8 -p pred8 -q a:
0.275278642938

vw -k -l .5 --l2 0.2 -d ctrain --cache_file /tmp/vw.cache  -f model9 --passes 100  --ignore o -q a:

So I need to write a script that will run VW and test a whole bunch of things in one go. 

0 4.10977945992
1e-06 3.91546724952
5e-06 2.4643048834
1e-05 1.78799338897
2e-05 0.797368151433
4e-05 0.263480718607
8e-05 0.262188559674 - THIS IS THE OPTIMAL HERE ... Still slightly worse than benchmark predictions.
0.0001 0.263604503018
0.0002 0.263992874231
0.004 0.271832601219
0.008 0.273979811739
0.01 0.275227863399
0.05 0.275184829586
0.1 0.275278642938
0.5 0.275278642938
1 0.275278642938

20140130
Next strategy take a simple feature set (just counts back for yesterday, last_week and last_year) and really optimise the personalisation side of things. 
Let us focus on predicting burglary based only on past burglaries.
Put onto a 200mx200m grid. I excluded cells that contained no crime (of any type) over the entire period - under the assumption that they lay outside the bounds of the division.
This leaves 1000 cells in which some type of crime has occured.
There are 7623 burglaries in division 1 in the 4 year period [2000-2003]. I am not going to break down crimes by premesis type or location or location. Just time back.
Run Java code. features:3 + areaID, instances:1095000 - an instance per cell for each day in [2001,2003]

Now split the data into training, validation, test  
head -n 365000 f_2000_2003_200mVW.txt > ctrain
head -n 730000 f_2000_2003_200mVW.txt | tail -n 365000 > cvalid
tail -n 365000 f_2000_2003_200mVW.txt > ctest

Generate baseline feature set with only feature being burglaries last year - and split in the same way - we only need a different training set.

Now run VW. 
invert_hash is empty if --bfgs is used ... The work-around is that the same hashing function is used so you can get the mapping from running sgd, but then actually use bfgs
Question... how does VW invert_hash deal with hash collisions anyway?
I can also get a mapping from id -> hash by using vw_varinfo

1) train the baseline model - the only predictive variable is the number of burglaries last year.

vw -k -l .5 -d btrain --cache_file /tmp/vw.cache  -f base --passes 50  --ignore a --bfgs --readable_model base.read --termination 0.00001 
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i base -p pred.base
rmse = 0.0731362023913

I'm a bit confused as to why the coefficient learnt is not 1 - ie number of burglaries = average # burglaries last year. .58 times burglaries last year seems odd - especially given more burglaries occured in 2001 than 2000. Something to think about.

Now run the single model version with 3 variables
vw -k -l .5 -d ctrain --cache_file /tmp/vw.cache  -f fully_coupled --passes 50  --ignore a --bfgs --readable_model fully_coupled.read --termination 0.00001 
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i fully_coupled -p pred.fully_coupled
0.0732283240212  - in other words a fraction worse!

20140203
Running some simulations to try to understand why we can't predict previous events as the long term average.

20140204
Lets just do a whole bunch of experiments with VW, learning with different grid sizes. In all the following experiments the data is restricted to residential burglaries.
IMPORTANT. DISCOVERED WITH VW AND NAMESPACES THERE MUST BE EXACTLY ONE SPACE AFTER FINAL FEATURE OTHERWISE ADDITIONAL FEATURES ARE INTERPRETED.

I should also do a little lable collision test - see what happens if I use the same label on two different namespaces.Rukshan thinks its ok - with namespace will be combined with featurename and then hashed

DATA generation 1000m grid. 5000m parent cells.
WARNING: no events for 1 periods in the data [59]
DATA LOADED
Category:ones levels:1
Area:area, size:806 ,Area:area5000, size:49 ,
Input periods:1460, Output periods:1095, Periods/instance:1
Target Areas:1011, Instance timewindows:1095, INSTANCES:1107045
Time buckets:2,Total category levels:1, area aggregation levels:2, FEATURES:4

DATA GENERATION, REPORTING AREA, PARENT AREA: SECTOR
WARNING: no events for 1 periods in the data [59]
DATA LOADED
Category:ones levels:1
Area:area_sec, size:32 ,Area:area, size:910 ,
Input periods:1460, Output periods:1095, Periods/instance:1
Target Areas:1139, Instance timewindows:1095, INSTANCES:1247205
Time buckets:2,Total category levels:1, area aggregation levels:2, FEATURES:4

for 1000mx1000m Grid

best l2 regularisation is .0001 for both quadratic and non-quadratic models (corresponds to model/pred 8)
rmse(pred_baseline)           = 0.12527
rmse(pred_b8)                 = 0.12576
rmse(.41*pred_b1+.59*pred_q8) = 0.12642
rmse(pred_q8)                 = 0.12706

for RA 
rmse(pred_b7)       = 0.119401001459
rmse(pred_q7)       = 0.12088993026
rmse(pred_baseline) = 0.118755181038

for 200m x 200m grid
WARNING: no events for 1 periods in the data [59]
DATA LOADED
Category:ones levels:1
Area:area, size:7882 ,Area:area5000, size:49 ,
Input periods:1460, Output periods:1095, Periods/instance:1
Target Areas:16296, Instance timewindows:1095, INSTANCES:17844120
Time buckets:2,Total category levels:1, area aggregation levels:2, FEATURES:4


20140210
I want to very clearly test this idea that we don't seem to be learning even the most simple relationship, because of the distribution of the target (and features)

Create a training, validation and test set based only on the amount of crime in the previous 365 days. 

vw -k -l .5 --l2 0 -d ctrain --cache_file /tmp/vw.cache  -f model1 --invert_hash model1.read  --passes 100 
learns y = -0.000687 + .670904x

vw -k -l .5 --l2 0 -d dtrain --cache_file /tmp/vw.cache  -f modelb --invert_hash modelb.read  --passes 100 train a model on fake data y = x + norm(0,.000001)
learns y = x

now apply these models to the validation data set.
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i model1 -p pred1
vw -t -d cvalid --cache_file /tmp/vw.cache.test -i modelb -p pred_base

20140218
Tried Poisson Regression on a subset of the data, gives slightly worse performance in terms of area under PAI curve than OLS. Poisson regression makes different assumptions about the relationship between the target variable (count) and the features, namely ... 

I think I need to substantially change the way the problem is represented, for information to be learned... Also I should think about directly optimising for ranking.

20140220
Validate models on year 2006. Build two training sets (200m grid, 2 temporal features). One uses data from 2001-2005 the other just 2005.
I need to modify my code such that it can label areas with higher up in the heirachy too.
I need to write code to calculate the area under the pai curve (in python so that it can be run)

Something went wrong with 200m grids - pai is .5 lets see if we can debug on 1000m grid

WARNING: no events for 2 periods in the data [59, 1520]
DATA LOADED
Category:ones levels:1
Area:area, size:837 ,Area:area5000, size:50 ,
Input periods:2556, Output periods:2191, Periods/instance:1
Target Areas:1011, Instance timewindows:2191, INSTANCES:2215101
Time buckets:2,Total category levels:1, area aggregation levels:2, FEATURES:4
python run_vw.py fVW1000_2005 fVW1000_2006 q l1 optimal is model 10
python run_vw.py fVW1000_2005 fVW1000_2006 b l2 optimal is model 10
python run_vw.py fVW1000_2005 fVW1000_2006 b l1 optimal is model 3
python run_vw.py fVW1000_2005 fVW1000_2006 q l2 optimal is model 9

optimising area under roc suggests 
ql1 best is 9 0.795
bl1 best is 9 0.798
ql2 best is 8 0.767
bl2 best is 9 0.77

TODO try elastic net - done
training over all pre2006, elastic net
b,m14 l1 = 1e-5,l2=1e-6
q,m19 l1 = .0001,l2=0

TODO try a model with all crime types as features (still burglary as target)
python run_vw.py fVW1kbcpre2006 fVW1kbc2006 b model 8 best
TODO run this: python run_vw.py fVW1kbcpre2006 fVW1kbc2006 q
TODO check sum of target variable is same as for previous case - should be since target is unchanged...(could do in interactive python with vw library)
THEY ARE NOT EQUAL - NEED TO DEBUG MY JAVA CODE. Use R to check how many burglaries there actually where in 2006 & 2005


TODO calculate the baseline

TODO I wonder if the reason models with more parameters perform worse than simpler models, even after regularization relates to the issue that we are optimising the rmse within the linear algorithm...

TODO check if bfgs can do l1 regularization (if so then use Rucshan's library to manually expand quadratic features and use bfgs)

TODO try aggregating over more days
TODO try personalizing at larger scale

20140226

VW1kall - target is residentual burglary, based on all crime, on a 1000m grid, cells are labeled by their grid and their 5000m grid. train is [2001,2004], validate = 2005, test = 2006

WARNING: no events for 2 periods in the data [59, 1520]
DATA LOADED
INPUT TARGET TOTAL:42368
Data consistant
Category:crime, levels:10
Category:prem, levels:10
Category:ones, levels:1
Area:area, size:1011 ,Area:area5000, size:53 ,
First period:365, Input periods:2556, Output periods:2191, Periods/instance:1
Target Areas:1011, Instance timewindows:2191, INSTANCES:2215101
Time buckets:2,Total category levels:21, area aggregation levels:2, FEATURES:84
Target total:36454.0

VW1kburg - target is residentual burglary, based only on residentual burglary on 1000m grid, cells are labeled by their grid and 5000m grid
WARNING: no events for 2 periods in the data [59, 1520]
DATA LOADED
INPUT TARGET TOTAL:42368
Data consistant
Category:crime, levels:1
Category:prem, levels:1
Category:ones, levels:1
Area:area, size:1011 ,Area:area5000, size:53 ,
First period:365, Input periods:2556, Output periods:2191, Periods/instance:1
Target Areas:1011, Instance timewindows:2191, INSTANCES:2215101
Time buckets:2,Total category levels:3, area aggregation levels:2, FEATURES:12
Target total:36454.0

VW200burg
WARNING: no events for 2 periods in the data [59, 1520]
DATA LOADED
INPUT TARGET TOTAL:42368
Data consistant
Category:crime, levels:1
Category:prem, levels:1
Category:ones, levels:1
Area:area, size:16296 ,Area:area5000, size:53 ,Area:area1000, size:1011 ,
First period:365, Input periods:2556, Output periods:2191, Periods/instance:1
Target Areas:16296, Instance timewindows:2191, INSTANCES:35704536
Time buckets:2,Total category levels:3, area aggregation levels:3, FEATURES:18
Target total:36454.0

VW200all
WARNING: no events for 2 periods in the data [59, 1520]
DATA LOADED
INPUT TARGET TOTAL:42368
Data consistant
Category:crime, levels:10
Category:prem, levels:10
Category:ones, levels:1
Area:area, size:16296 ,Area:area5000, size:53 ,Area:area1000, size:1011 ,
First period:365, Input periods:2556, Output periods:2191, Periods/instance:1
Target Areas:16296, Instance timewindows:2191, INSTANCES:35704536
Time buckets:2,Total category levels:21, area aggregation levels:3, FEATURES:126

TODO check if the --ignore flags actually work and how they work.
TODO check if the quadratic features work the way I think they do (ie can I use -q multiple times)
TODO check if bfgs supports l1 regularization
TODO if it does ask Rukshan for script to manually generate quadratic features.

# testing holdout stuff with VW
vw -k -l .5 --l2 0 -d train.dat --cache_file /tmp/vw.cache  -f model1 --invert_hash model1.read --passes 500
vw -t -d test.dat --cache_file /tmp/vw.cache.test -i model1 -p pred1

having holdout on can lead to early stopping - although that can be prevented by setting --early-terminate to a number >= number of passes
The intent is to help reduce overfitting - by stopping early. I don't see why Rukshan was getting errors of 0 though, even on the holdout set. - maybe if there is a substantial chance of the target and all features being 0, then the holdout set could consist entirely of such rows. However, given a reasonable sized data set (and thus holdout set) this would decay according to the binomial distribution so seems very unlikely. It would be nice if it were possible to inspect the holdout set.

20140331
Lets try infering a very simple hierachical Bayes model.

First up. Goal is to predict residentual burglary. 
I am going to use the years 2000-2004 to train and validate models and 2005 to test them.

Try a number of grid sizes. 

20140409
Make each period 1 week.
Total data used 2000-2004. Training is [2001-2003]. Validate is [2004]

On the events5000
DATA LOADED
INPUT TARGET TOTAL:29914
Data consistant
Category:crime, levels:10
Category:prem, levels:10
Category:ones, levels:1
Area:area, size:53 ,Area:areaall, size:1 ,
First period:52, Input periods:260, Output periods:208, Periods/instance:1
Target Areas:53, Instance timewindows:208, INSTANCES:11024
Time buckets:3,Total category levels:21, area aggregation levels:2, FEATURES:126
Target total:23910.0
DONE

Something is wrong with target generation in Java ...
python ../pai.py [VW5kallqa4.pred,VW5kallignorea1.pred,VW5kallkeepo1.pred, VW5kallvalid.target ,53][VW1kallqa10.pred,VW1kallvalid.target,1011]

Best 1k by rmse
VW1kallignorea3.pred
VW1kallqa10.pred
VW1kallkeepo12.pred
mv VW1kbignorea1.pred baseline1k.pred
VW1kbdignorea1.pred - baseline for daily


Best 5k by rmse
VW5kallqa4.pred
VW5kallignorea1.pred
VW5kallkeepo1.pred
VW5kbignorea1.pred -> baseline5k.pred


python ../pai.py [baseline1k.pred, VW1kallignorea3.pred,VW1kallqa10.pred, VW1kallvalid.target,1011]
python ../pai.py [baseline5k.pred,VW5kallqa4.pred,VW5kallignorea1.pred, VW5kallvalid.target ,53]

python ../pai.py [baseline1k.pred, VW1kallignorea3.pred,VW1kallqa10.pred, VW1kallvalid.target,1011][VW5kallqa4.pred, VW5kallvalid.target ,53]

python ../pai.py [baseline1k.pred, VW1kallvalid.target,1011][VW1kbdignorea1.pred, VW1kbdvalid.target ,1011]

Daily predictions do worse (even on the baseline)



Crime and Punishment
====================================================================================

Initial exploratory analysis
------------------------------------------------------------------------------------
```{r load-data, message=FALSE,warning=FALSE}
#load the data
data = read.table("/home/finn/phd/data/rawwithgeo.txt", header=TRUE, sep="\t")
```

I have categorized the crimes based on the first two digits of the *ucr1* column and maped the resulting factor levels to descriptive strings based on the information provided at http://policereports.dallaspolice.net/ucr.htm. The meaning of a couple of two digit codes, that occured in our data but were not listed on the above website, were inferred from viewing a summary of the descriptions provided for these cases in the data. I then constructed 4 groupings of these categories. Currently they are:

- Light: criminal mischief, disorderly, vice, fence and found property
- Violent: assult, agg assult, murder, rape and robbery
- Property: theft, burglary, un-authorized use of a motor vehicle
- Other: all other crime categories 
```{r categorize-crime, message=FALSE,warning=FALSE}
#categorize the crime
data$crime <- as.factor(substr(data$ucr1,1,2))
library(plyr)
data$crime <- revalue(data$crime, c("01"="murder", "02"="rape","03"="robbery","04"="agg_assult","05"="burglary","06"="theft","07"="uumv","08"="assult","09"="arson","10"="forge","11"="fraud","12"="embezzlement","13"="fence","14"="crim_misch","16"="vice","17"="sex off","18"="drugs","20"="child","24"="disorderly","26"="other","29"="runnawy","32"="flid","33"="injured","34"="injured2","39"="att_suicide","40"="death","41"="missing_person","42"="lost","43"="found"))

light <- c("crim_misch","disorderly","vice","fence","found") #matches the broken windows paper by Gregorio?
violent <- c("assult","agg_assult","murder","rape","robbery") 
property <- c("theft","burglary","uumv")


data$crime_category <-"other"
data$crime_category[data$crime %in% light] <- "light"
data$crime_category[data$crime %in% violent] <- "violent"
data$crime_category[data$crime %in% property] <- "property"
data$crime_category <- as.factor(data$crime_category)
```


How many crimes do we have in each category
```{r}
crime_by_category <- as.data.frame(prop.table(table(data$crime_category)))
crime_by_category <- crime_by_category[with(crime_by_category, order(-Freq)),] # sort to order by frequency
barplot(crime_by_category$Freq,names.arg=crime_by_category$Var1,ylab="proportion of crime",main="proportion of crime by category")

crime_by_type <- as.data.frame(prop.table(table(data$crime)))
crime_by_type <- crime_by_type[with(crime_by_type,order(-Freq)),]
crime_by_type <- crime_by_type[1:20,]
barplot(crime_by_type$Freq,names.arg=crime_by_type$Var1,ylab="proportion of crime",main="proportion of crime by ucr code (top 20)",las=2)
```

Correlation between light and violent crime accross reportingareas. 
```{r visualize-relationship-between-crime-types, echo=FALSE, message=FALSE,warning=FALSE}
library(sqldf)
library(reshape2)
# aggregate by reporting-area
by_reporting_area <- sqldf('select reportingarea,crime_category,count(1) as instances from data group by reportingarea,crime_category')
by_ra <- dcast(by_reporting_area, reportingarea ~ crime_category, value.var='instances', fill=0)
plot(by_ra$violent~by_ra$light,main="violent crime vs light crime in a reporting area")

```

Look at the categorizations of targets as encoded by the 'property attack code' field. 
```{r targarts} 
targets = c("100"="misc","101"="industrial","102"="store","103"="store","104"="misc","105"="store","106"="store","107"="store","108"="store","109"="store","110"="store","111"="pharmacy","112"="pharmacy","113"="store","114"="store","115"="store","116"="store","117"="store","118"="store","119"="store","120"="store","121"="store","122"="store","123"="store","124"="gas station","125"="store","126"="store","127"="store","128"="bar","129"="misc","130"="industrial","131"="industrial","132"="industrial","133"="trailer","134"="misc","135"="store","136"="apartment","137"="hotel room","138"="misc","139"="church","140"="store","141"="store","142"="misc","143"="misc","144"="mall","145"="mall","146"="mall","200"="business","201"="store","202"="food","203"="store","204"="store","205"="vacant lot","206"="misc","207"="misc","208"="store","209"="industrial","210"="misc","211"="store","212"="store","213"="store","214"="store","215"="store","216"="misc","217"="industrial","218"="industrial","219"="industrial","220"="storage","221"="industrial","222"="misc","223"="garage","224"="trailer","225"="food","226"="bank","227"="bank","228"="business","229"="school","230"="industrial","231"="store","232"="misc","233"="school","234"="industrial","235"="storage","236"="misc","237"="industrial","238"="bank","239"="school","300"="food","301"="food","302"="food","303"="food","304"="food","305"="food")

targets2 = c("400"="business","401"="business","402"="business","403"="business","404"="misc","405"="station","406"="industrial","500"="apartment","501"="house","502"="house","503"="apartment","504"="trailer","505"="accom other","506"="garage","507"="accom other","508"="recreation","509"="misc","510"="driveway/yard","511"="house","600"="recreation","601"="recreation","602"="apartment","603"="misc","604"="recreation","605"="recreation","606"="recreation","607"="recreation","608"="misc","609"="recreation","610"="park","611"="recreation","612"="recreation","613"="recreation","700"="business","701"="accom other","702"="business","703"="pharmacy","704"="pharmacy","705"="store","706"="business","707"="accom other","708"="business","709"="business","710"="business","711"="recreation","801"="govt","802"="govt","803"="govt","804"="govt","805"="govt","806"="govt","807"="govt","808"="govt","809"="govt","810"="store","811"="recreation","812"="school","813"="business","900"="misc","901"="public street","902"="business","903"="misc","904"="misc","905"="misc","906"="misc","907"="vacant house","908"="vacant lot","909"="storage","910"="public street","911"="misc","912"="park","913"="misc","914"="trailer","915"="misc","916"="storage","917"="park","918"="park","919"="misc","920"="parking","921"="parking","922"="parking","923"="parking","924"="parking","925"="parking","926"="misc","927"="parking","928"="parking","929"="parking","930"="parking","931"="parking","932"="parking","933"="parking","934"="parking","935"="parking","936"="parking","937"="parking","938"="garage")


targets <- c(targets,targets2) # there seems to be a bug that doesn't let me assign more than a certain number of elmements to a list in one go.
rm(targets2)
 
data$pa <- as.factor(data$property_attack_code)
data$target_type <- revalue(data$pa,targets)
crime_by_target <- as.data.frame(prop.table(table(data$target_type)))
crime_by_target <- crime_by_target[with(crime_by_target, order(-Freq)),] # sort to order by frequency
top <- crime_by_target # top 30 location types account for 90% of crime
op <- par(mar = c(15,4,4,2) + 0.1)
barplot(top$Freq,names.arg=top$Var1,ylab="proportion of crime",main="proportion of crime by target type",las=2)
par(op) ## reset
rm(top)
data$pa <- NULL

# lets say we wanted 30 location types. So we took the top 30 as above and then compared all the other codes to how similar they are to existing codes in terms of where the crimes are comitted, what time of day, and what type of crime. Could calculate a similarity score and then apply a graph clustering algorithm...


```

I calculated a number of temporal variables from the *rep_date*, *starttime*, *endtime* and *dispatchtime* variables. They are:
- year the year in which the event took place
- month: the month of year in which the event took place
- dow: the day of week in which the event took place
- hourofday: the hour of day - based on dispatchtime
- responcetime: the difference between starttime and dispatchtime. Where negative, 24 hours was added to deal with cases on either side of the midnight boundary
- onscenetime: the difference between endtime and starttime. Where negative, 24 hours was added to deal with cases on either side of the midnight boundary

```{r compute-temporal-varaibles, message=FALSE,warning=FALSE}
library(lubridate)
data$year <- as.integer(substr(data$rep_date,7,10))
data$starttime2 <- hms(data$starttime)
data$endtime2 <- hms(data$endtime)
data$dispatchtime2 <- hm(data$dispatchtime)
data$responcetime <- as.duration(data$starttime2 - data$dispatchtime2)
data$onscenetime <- as.duration(data$endtime2 - data$starttime2)
data$onscenetime[!is.na(data$onscenetime) & data$onscenetime < 0] <- data$onscenetime[!is.na(data$onscenetime) & data$onscenetime < 0]+as.duration(hours(24))
data$responcetime[!is.na(data$responcetime) & data$responcetime < 0] <- data$responcetime[!is.na(data$responcetime) & data$responcetime < 0]+as.duration(hours(24))
data$date <- mdy(as.character(data$rep_date))
data$dow <-wday(data$date)
data$month <-month(data$date) 
data$hourofday <- hour(data$dispatchtime2)
```

Basic histograms of temporal variables
```{r}
barplot(table(data$hourofday),las=3,xlab="hour of day",ylab="number of crimes",main="crime counts by hour of day")
barplot(table(data$dow),las=3,xlab="day of week",ylab="number of crimes",main="crime counts by day of week")
barplot(table(data$month),las=3,xlab="month of year",ylab="number of crimes",main="crime counts by month of year")
barplot(table(data$year),las=3,xlab="year",ylab="number of crimes",main="crime counts by year")

tmp <- data[data$responcetime < 720*60,"responcetime"] #ignore times longer than 12 hours
hist(tmp/60, xlab="responce time (minutes)",main="distribution of responce times")
tmp <- data[data$onscenetime<180*60,"onscenetime"] #ignore times longer than 3 hours
hist(tmp/60,xlab="minutes spent on scene",main="distribution of time spent on scene")
rm(tmp)

```


These plots summarize the relationship between responce-time, on-scene-time and a number of other variables. There was less correlation with other variables than I would have anticipated.
```{r visualize-temporal-variables, message=FALSE,warning=FALSE}
plot(responcetime/(60)~crime_category,data=data,varwidth=TRUE,outline=FALSE, xlab="crime type",ylab="responce time (minutes)",main="responce time vs crime type")
plot(onscenetime/60~crime_category,data=data,varwidth=TRUE,outline=FALSE,xlab="crime type",ylab="time spent on scene (minutes)",main="on scene time vs crime type")

# responce time by time of day and day of week
boxplot(responcetime/60 ~ dow,data=data,outline=FALSE,main="responce time by day of week")
boxplot(responcetime/60 ~hourofday,data=data,outline=FALSE,main="responce time by hour of day") 

data$division <-as.integer(substr(data$beat,1,1))
data$sector <- as.integer(substr(data$beat,1,2))

#by geographical area
boxplot(responcetime/60~division,data=data,varwfenceidth=TRUE,outline=FALSE,main="responce time by division") 
boxplot(responcetime/60~sector,data=data,varwidth=TRUE,outline=FALSE,main="responce time by sector") 

```




Testing for variation in beat and reporting area lables
-----------------------------------------------------------------------------------------------------------
I have calculated the geographical centroid of each beat, for each year as the median lat and longitude of the points in that beat that year. Then, for each year, we can look at the mean difference in the centroids, accross all the beats, from the previous year. This gives an indication of how much change there has been overall in how the lables map to geographical areas. The process for reporting area is the same. I'd say we can conclude from this that something definately happend, at least with the beat labels we have, in 2003 and 2008/2009, but the reporting area appears relatively constant, all the way up to 2010.

```{r testing-centroid-variation,  message=FALSE,warning=FALSE}
sub <- data[data$geo_conf > .8,] # restrict to data with confident geo

f_diff <- function(vect) {
  d <- diff(vect)
  d <- c(NA,d)
  return(d)
}

centroid_change <- function(median_lat,median_lon){
  colnames(median_lat) <- c("beat","year","lat") #call the geographic region beat in this code so as to not have to change is as we switch region we look at.
  colnames(median_lon) <- c("beat","year","lon")
  meads <- merge(median_lat,median_lon,by=c("year","beat"))
  meads <- meads[meads$year <=2011,]

  # order by beat, then year. 
  meads <- meads[with(meads, order(beat,year)),]
  # we want a count of how many years a beat has data for
  count_by_year <- aggregate(lat~beat,meads,length)
  colnames(count_by_year) <- c("beat","year_count")
  meads <- merge(meads,count_by_year,by="beat")
  rm(count_by_year)
  meads <- meads[meads$year_count == 11,] #only consider regions that have at least 1 data point for all the years
  # cacluate a column lat_diff and a column lon_diff by applying f_diff 
  meads$lat_diff <- f_diff(meads$lat) # the value for the year 2000 will always be NA or from a different beat but we will throw it out later ...
  meads$lon_diff <- f_diff(meads$lon)
  meads$dist_diff <- sqrt(meads$lat_diff^2+meads$lon_diff^2)*100*110 #to get in meters one degree of lat or lon assumed equal to 110km
  meads <- meads[meads$year > 2000,]
  return(meads)
}

beat_median_lat <- aggregate(lat~beat+year,sub, median)
beat_median_lon <- aggregate(lon~beat+year,sub,median)
beat_change_detail <- centroid_change(beat_median_lat,beat_median_lon)
beat_change <- aggregate(dist_diff~year,beat_change_detail,mean)
plot(beat_change$year,beat_change$dist_diff,ylim=c(0,330),xlab="year",ylab="~mean difference in median lat-lon from previous year (meters)",main="beat centroid variation by year",pch=19)
diff_2003 <- beat_change_detail[beat_change_detail$year==2003,]
diff_2005 <- beat_change_detail[beat_change_detail$year==2005,]
diff_2008 <- beat_change_detail[beat_change_detail$year==2008,]
```
We can look in more detail at what happened in 2003 and 2008 by looking at the distribution of changes in beat centroid. The following plots show these distributions for 2003, 2005 (as a control when nothing seemed to be happening) and 2008.
```{r testing-centroid-variation-plots,  message=FALSE,warning=FALSE}
hist(diff_2003$dist_diff,xlab="change in beat centroid (meters)",main="distribution of changes in beat centroid, 2002-2003",breaks=20)
hist(diff_2005$dist_diff,xlab="change in beat centroid (meters)",main="distribution of changes in beat centroid, 2004-2005",breaks=20)
hist(diff_2008$dist_diff,xlab="change in beat centroid (meters)",main="distribution of changes in beat centroid, 2007-2008",breaks=20)
```
And doing the same for reporting area
```{r testing-ra-centroid-variation,  message=FALSE,warning=FALSE}
ra_median_lat <- aggregate(lat~reportingarea+year,sub, median)
ra_median_lon <- aggregate(lon~reportingarea+year,sub,median)
ra_change_detail <- centroid_change(ra_median_lat,ra_median_lon)
ra_change <- aggregate(dist_diff~year,ra_change_detail,mean)
plot(ra_change$year,ra_change$dist_diff,ylim=c(0,330),xlab="year",ylab="~mean difference in median lat-lon from previous year (meters)",main="reporting area centroid variation by year",pch=19)
plot(ra_change$year,ra_change$dist_diff,xlab="year",ylab="~mean difference in median lat-lon from previous year (meters)",main="reporting area centroid variation by year (closeup)",pch=19)

```

So assuming we might work with reporting area... lets explore reporting area more. Lets assume we will exclude from prediction reporting areas with less than 100 crimes. 
```{r}
counts_by_ra = aggregate(ones~reportingarea,data=data,sum)
hist(counts_by_ra$ones,breaks=20,las=2)
```





I have a python script, temporal2.py that, for each reporting area, calculates percentage of time a crime occures given the number of days since the last crime.
```{r load-temporal-decay, message=FALSE,warning=FALSE}
by_days_since = read.table("/home/finn/phd/data/bydaysince.txt",header=TRUE,sep="\t")
days_dist = aggregate(num_days~days_since, data = by_days_since,sum)
plot(days_dist$days_since,days_dist$num_days,xlim=c(0,28),xlab="day since last crime",ylab="count",main="Distribution of days since last crime")
decay = aggregate(per_crime_days_ad~days_since,data = by_days_since,mean)
decay = decay[decay$days_since < 29,]
plot(decay$days_since,decay$per_crime_days_ad,xlab = "days since last light crime",ylab="freq of light crime above background", main="Freq of light crime above background vs Days since last light crime")
```

```{r}
## Summarizes data.
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    require(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}

```

```{r}
avg= read.table("/home/finn/phd/data/bydaysince2.txt",header=TRUE,sep="\t")
avg_burg= read.table("/home/finn/phd/data/bydaysinceburg.txt",header=TRUE,sep="\t")
avg_violent = read.table("/home/finn/phd/data/bydaysinceviolent.txt",header=TRUE,sep="\t")
avg_rape =  read.table("/home/finn/phd/data/bydaysincerape.txt",header=TRUE,sep="\t") # to noisy
avg_robb =  read.table("/home/finn/phd/data/bydaysincerobb.txt",header=TRUE,sep="\t")
avg_uumv =  read.table("/home/finn/phd/data/bydaysinceuumv.txt",header=TRUE,sep="\t")
avg_theft =  read.table("/home/finn/phd/data/bydaysincetheft.txt",header=TRUE,sep="\t")
avg_assult = read.table("/home/finn/phd/data/bydaysinceassult.txt",header=TRUE,sep="\t")
avg_mis = read.table("/home/finn/phd/data/bydaysincemis.txt",header=TRUE,sep="\t")
avg_found = read.table("/home/finn/phd/data/bydaysincefound.txt",header=TRUE,sep="\t")
avg$type="light"
avg_burg$type="burglary"
avg_violent$type = "violent"
avg_rape$type="rape"
avg_robb$type="robbery"
avg_uumv$type="uumv"
avg_theft$type="theft"
avg_assult$type="assult"
avg_mis$type="mischief"
avg_found$type="found"



all <- rbind(avg_assult,avg_burg,avg_theft,avg_mis)

all$per <- 100*(all$num_crimes-all$background)/all$background
s <- summarySE(all,measurevar="per",groupvars=c("days_since","type"))
shead <- s[s$days_since < 15,]

library(ggplot2)

ggplot(shead, aes(x=days_since, y=per,colour=type)) + 
    geom_errorbar(aes(ymin=per-ci, ymax=per+ci), width=.1) +
    geom_point(shape=19,size=4) +
    xlim(0,15)+ylim(-20,50)+
    xlab("days since last crime")+
    ylab("% change in expected number of crimes")+
    ggtitle("% change in expected number of crimes vs days since last crime")


avg$per <- 100*(avg$num_crimes-avg$background)/avg$background
s <- summarySE(avg,measurevar="per",groupvars=c("days_since"))
shead <- s[s$days_since < 36,]
ggplot(shead, aes(x=days_since, y=per)) + 
    geom_errorbar(aes(ymin=per-ci, ymax=per+ci), width=.1) +
    geom_point(shape=19,size=4) +
    xlim(0,15)+ylim(-20,30)

```



Lets check out the distribution of the distances between centroids
```{r centroid-distances, message=FALSE,warning=FALSE}
distances = read.table("/home/finn/phd/data/distances.txt",header=FALSE,sep=",")
hist(distances$V1,xlab = "distance (meters)",main ="Histogram of distances between all pairs of reporting areas")

# just for comparision - what does this look like on a grid?
test = merge(1:10,1:10,by=NULL)
test = merge(test,test,by=NULL)
test$dist = sqrt((test$x.x - test$y.x)^2 + (test$x.y - test$y.y)^2)

```

```{r}
# explore correlations
correl <- read.table("/home/finn/phd/data/cor.txt",header=TRUE,sep="\t")

```
